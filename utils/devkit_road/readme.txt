###########################################################################
#            THE KITTI VISION BENCHMARK SUITE: ROAD BENCHMARK             #
#              Jannik Fritsch   Tobias Kuehnl   Andreas Geiger            #
#                Honda Research Institute Europe GmbH                     #
#                         Bielefeld University                            #
#                    Karlsruhe Institute of Technology                    #
#                             www.cvlibs.net                              #
###########################################################################

Update February 2015:
Now ground truth is stored as unsigned char color PNG
- first/R plane contains valid evaluation area
  -> Exclusion of road areas that are not relevant for evaluation
- third/B plane contains road area ground truth
  
  
Original version March 2014:
This file describes the KITTI road benchmark, consisting in total of 
289 training and 290 test images. It contains three different
categories of road scenes:
  uu  - urban unmarked (98/100)
  um  - urban marked (95/96)
  umm - urban multiple marked lanes (96/94)
Ground truth has been generated by manual annotation of the images and 
is available for two different road terrain types:
  road - the road area, i.e, the composition of all lanes
  lane - the ego-lane, i.e., the lane the vehicle is currently driving on
         (only available for category "um")
The ground truth is provided for training images only. Results on the
testing data set can be uploaded to the KITTI web server for evaluation.

NOTE: WHEN SUBMITTING RESULTS, PLEASE NAME THE FILES ACCORDING TO THE 
GROUND TRUTH (SEE BELOW), USING THE FILE NAMES:
  <cat_type>_000000.png TO <cat_type>_000xxx.png!
Valid combinations are [um_road, umm_road, uu_road, um_lane].
For example, the first file for category "um" of type "road" is 
um_road_000000.png and the last file is um_road_000095.png.
THE RESULT FILES MUST BE TRANSFORMED INTO THE BIRDS EYE VIEW SPACE 
BEFORE SUBMITTING (THERE IS A SCRIPT IN THE DEVKIT, SEE BELOW).
CREATE A ZIP ARCHIVE OF THE RESULTS. 
PLEASE MAKE SURE RESULT FILES ARE IN THE ROOT FOLDER.


File description and data format of benchmark data:
===================================================

The folders testing and training contain the color video images in
the sub-folder image_2 (left image). All input
images are saved as unsigned char PNG images. Filenames are
composed of image category (um, umm or uu) and a 6-digit image index:

 - <cat>_xxxxxx.png

Here xxxxxx is running from 0 to the number of files in that category. 
Optionally, you can download corresponding right color images (image_3) 
or grayscale stereo data (image_0 + image_1).
However, the reference images, for which results must be provided, 
are always the left images (image_2).

Corresponding ground truth annotations for the training data can be 
found in the folder gt_image_2. Ground truth is available as 
unsigned char PNG images. The first channel contains valid evaluation
areas and the third channel contains the ground truth.
The ground truth file names have two prefixes, 
category and type (see also above):

 - <cat>_<type>_xxxxxx.png

For all categories (um, umm, uu) the type "road" is available.
For category "um" also the type "lane" is available.
NOTE: Generated files resulting from processing the benchmark test data 
should follow the same naming convention.

Corresponding calibration files for every frame can be found in the calib 
directory of both training and testing data. These are required for 
transforming perspective images/results into Birds Eye View space.

Each calibration file contains the following matrices (in row-major order):
P0 (3x4): Projection matrix for left grayscale camera in rectified coordinates
P1 (3x4): Projection matrix for right grayscale camera in rectified coordinates
P2 (3x4): Projection matrix for left color camera in rectified coordinates
P3 (3x4): Projection matrix for right color camera in rectified coordinates
R0_rect (3x3): Rotation from non-rectified to rectified camera coordinate system
Tr_velo_to_cam (3x4): Rigid transformation from Velodyne to (non-rectified) camera coordinates
Tr_imu_to_velo (3x4): Rigid transformation from IMU to Velodyne coordinates
Tr_cam_to_road (3x4): Rigid transformation from (non-rectified) camera to road coordinates

For instance, to transform a point in road coordinates to the left color image
(= image_2 for which semantic labels are provided in label_2), the 3D point X
transforms to pixel x as

x = P2 * R0_rect * Tr_cam_to_road^-1 * X

where R0_rect and Tr_cam_to_road have been extended to 4x4 matrices by adding
a fourth row (with 1 as the last element and zeros elsewhere) and a fourth
column (for R0_rect only).

There is an enclosed python script for the transformation using the 
calibration data:
 Usage: python transform2BEV.py <InputFiles> <PathToCalib> <OutputPath>
        (see script for further details)

Note: The utility functions in this development kit are based on Python.
MATLAB and C++ utility functions for using the kitti data are 
provided in the development kit of the stereo/flow benchmark.

Data format for result submission:
==================================
For submission, results must be transfered into the Birds Eye View (BEV).
For the KITTI Road Benchmark the setup for the BEV is fixed:
 - The extent of the BEV spans a metric space from [-10m, 10m] laterally 
   and [6m, 46m] longitudinally.
 - Using a metric resolution of 0.05m this results in a 
   BEV size of [800px, 400px].

Note that the submitted results will be evaluated within BEV space.
There is an enclosed python script which does the transformation using 
the above mentioned parameters, see "python/transform2BEV.py" 
for further details.

Results can be submitted as binary map [0,255] or confidence 
map [0..255] in BEV. For submissions of confidence maps a working 
point (WP) will be computed by the evaluation script by finding 
a threshold maximizing the F1-Measure. Demonstrating the evaluation, 
there is a enclosed python script (see next section).


Python Example Code:
====================

We also provide some helper functions and two example scripts in the 
subfolder 'python' of this development kit. For transparency we have 
included the KITTI evaluation code as well.

1. Example: 
   "Baseline classifier and Perspective Evaluation on training data"
   Run the script to generate classification results on the training
   data by learning a baseline classifier from ground truth. 
   Performs the pixel-based evaluation in perspective space 
   (on the training data).

   Usage: python simpleExample_evalTrainResults.py <datasetDir>


2. Example: 
   "Baseline classifier on testing data and Conversion to BEV"
   Run the script to generate classification results on the training
   data by learning a baseline classifier from ground truth.
   Performs conversion of perspective results to BEV for upload
   to the server.

   Usage: python simpleExample_transformTestResults2BEV.py <datasetDir>
   
   
Python Dependencies:
====================
The python files in this development kit where created with python 2.7 
and depend on the following libraries:
1) Numpy (tested with 1.51)
2) OpenCV (tested with 2.46)

